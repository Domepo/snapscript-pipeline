# ðŸŽ¬ Video2Scriptâ€‘Pipeline

![Alt Text](assets\output.gif)
*Vom Vorlesungsâ€‘Video zum strukturierten Skript & PDF â€“Â vollautomatisch.*
## Ãœber das Projekt

Video2Script verwandelt aufgezeichnete Vorlesungen (oder andere Bildschirmâ€‘/Kameraâ€‘Videos) in ein sauber gegliedertes Skriptâ€¯inkl.Â Bildern, SchlagwÃ¶rtern und fertigem PDFâ€‘Export.
Dazu kombiniert der Workflow mehrere KIâ€‘Modelle und Openâ€‘Sourceâ€‘Tools:

1. **Whisper** transkribiert den Ton des Videos.
2. **YOLOâ€‘v11** erkennt Folienâ€‘ und Tafelâ€‘Abschnitte im Videoâ€‘Stream.
3. **Ollamaâ€¯+â€¯GemmaÂ 3:27bÂ âš™ï¸Ž** analysiert Bilder & Text, erzeugt Keywords und poliert das Skript.
4. **Typst** rendert das Markdownâ€‘Skript zu einem hÃ¼bschen PDF.
5. **Streamlit** bildet die interaktive BenutzeroberflÃ¤che.

Das Ergebnis ist ein lesefertiges Skript (MarkdownÂ +Â PDF), das Lehrende direkt an Studierende verteilen kÃ¶nnen.

---

## Table of Contents

* [Ãœber das Projekt](#Ã¼ber-das-projekt)
* [Verwendete Libraries](#verwendete-libraries)
* [Beispielâ€‘Output](#beispiel-output)
* [Installation](#installation)

  * [Dockerâ€‘Quickstart](#docker-quickstart)
  * [Lokale Pythonâ€‘Umgebung](#lokale-python-umgebung)
* [AusfÃ¼hrung](#ausfÃ¼hrung)
* [Dokumentation](#dokumentation)

---

## Verwendete Libraries

| Kategorie                         | Wichtigste Pakete                                  |
| --------------------------------- | -------------------------------------------------- |
| **Speechâ€‘toâ€‘Text**                | `openai-whisper`                                   |
| **Bildâ€‘/Objekterkennung**         | `ultralytics` (YOLOÂ v8)                            |
| **Largeâ€‘Languageâ€‘Model**          | `ollama`, `gemma3:27b`                             |
| **Dokumentâ€‘Rendering**            | `typst`                                            |
| **UIÂ /Â Server**                   | `streamlit`, `fastapi`                             |
| **Computer VisionÂ &Â Bildanalyse** | `opencv-python`, `scikit-image`, `imagehash`       |
| **DataÂ &Â Utilities**              | `pandas`, `numpy`, `tqdm`, `psutil`, `coloredlogs` |

*Die vollstÃ¤ndige Liste aller Pythonâ€‘AbhÃ¤ngigkeiten findest du in [`requirements.txt`](requirements.txt).*

---

## Beispielâ€‘Output

```text
/data/pdf/astro_vorlesung.pdf
```

<p align="center">
  <img src="assets\example.png">
</p>

---

## Installation
Wir empfehlen die Nutzung von Docker
### Dockerâ€‘Quickstart

```bash
# OllamaÂ â€“Â benÃ¶tigt das Gemmaâ€‘3â€‘Modell (â‰ˆÂ 19â€¯GB)
docker run -d \
  -v ollama:/root/.ollama \
  -p 11434:11434 \
  --name ollama ollama/ollama
# optional: Modell vorab laden
docker exec -it ollama ollama run gemma3:27b

# Video2Scriptâ€‘Container bauen & starten
docker build -t video2script .
docker run -d --name video2script \
  --link ollama:ollama \
  -p 8501:8501 \
  -v $(pwd)/data:/app/data video2script
```
Der Container ist nun Ã¼ber `localhost:8501` errecihbar
### Lokale Pythonâ€‘Umgebung

```bash
python -m venv venv
source venv/bin/activate  # Windows: .venv\Scripts\activate
pip install --upgrade pip
pip install -r requirements.txt

# ffmpeg & system libs werden unter Linux via apt installiert (siehe Dockerfile)
```

---

## AusfÃ¼hrung

1. **Streamlitâ€‘App starten** (nur nÃ¶tig auÃŸerhalb des Dockerâ€‘Containers):

   ```bash
   streamlit run app/main.py
   ```
2. Im Browser `http://localhost:8501` Ã¶ffnen.
3. Video (MP4) hochladen und den Fortschrittsbalken abwarten.
4. PDF herunterladen oder Skriptâ€‘/Keywordâ€‘Vorschau anzeigen.

---

## Dokumentation

* Technischer Ablauf & Codeâ€‘Dokumentation findest du in den Docstrings der ModuleÂ unter `app/*`.
* **Datenbankâ€‘Schema** wird beim ersten Lauf automatisch unter `presentation_slides.db` erzeugt (SQLite).
* FÃ¼r tiefergehende Infos lies die Kommentare im [Dockerfile](dockerfile) und im Sourceâ€‘Code.

> **Tipp:** Aktiviere `logging` auf LevelÂ `DEBUG`, um detaillierte Ausgaben Ã¼ber die Bildâ€‘/Textâ€‘VerknÃ¼pfung zu erhalten.

---

Â©Â 2025Â â€“Â MITÂ Licence
